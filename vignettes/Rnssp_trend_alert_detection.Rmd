---
title: "Anomaly Detection and Trend Classification"
author:
output:
   rmarkdown::html_document:
     toc: true
     toc_float:
       toc_collapsed: true
     toc_depth: 2
     number_sections: true
     theme: readable
vignette: >
  %\VignetteIndexEntry{Anomaly Detection and Trend Classification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = FALSE
)
```

# Introduction {-}
In this tutorial, we describe how to perform anomaly detection and trend classification analysis using time series data from NSSP-ESSENCE. This vignette uses time series data from NSSP-ESSENCE data source for the CLI CC with CLI DD and Coronavirus DD v2 definition, limiting to ED visits (Has been Emergency = "Yes").

We start this tutorial by loading the `Rnssp` package and all other necessary packages.

```{r setup}
library(Rnssp)
library(ggplot2)
library(ggthemes)
```

The next step is to create an NSSP user profile by creating an object of the class `Credentials`. 

```{r load, echo=FALSE, eval=TRUE}
myProfile <- readRDS("../myProfile.rds")
```


```{r create_profile, eval=FALSE}
# Creating an ESSENCE user profile
myProfile <- create_profile()

# save profile object to file for future use
# save(myProfile, "myProfile.rda") # saveRDS(myProfile, "myProfile.rds")
# Load profile object
# load("myProfile.rda") # myProfile <- readRDS("myProfile.rds")
```

# Data Pull from NSSP-ESSENCE
With the NSSP `myProfile` object, we authenticate to NSSP-ESSENCE and pull in the data using the Time series data table API.
```{r data_pull, collapse=TRUE}
url <- "https://essence2.syndromicsurveillance.org/nssp_essence/api/timeSeries?endDate=20Nov20&ccddCategory=cli%20cc%20with%20cli%20dd%20and%20coronavirus%20dd%20v2&percentParam=ccddCategory&geographySystem=hospitaldhhsregion&datasource=va_hospdreg&detector=probrepswitch&startDate=22Aug20&timeResolution=daily&hasBeenE=1&medicalGroupingSystem=essencesyndromes&userId=2362&aqtTarget=TimeSeries&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&nonZeroComposite=false&removeZeroSeries=true&startMonth=January&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&startMonth=January&nonZeroComposite=false"

# Data Pull from NSSP-ESSENCE
api_data <- get_api_data(url, profile = myProfile)
df <- api_data %>% 
  pluck("timeSeriesData")

# glimpse(df)

```

First, let's group the data by HHS regions
```{r hhs}
# Aggregating data to national level
df_hhs <- df %>%
  group_by(hospitaldhhsregion_display)
```

# Anomaly Detection
As of `r format(Sys.Date(), "%B %d, %Y")`, the `Rnssp` package implements two anomaly detection algorithms:

## Exponentially Weighted Moving Average (EWMA)
The Exponentially Weighted Moving Average (EWMA) compares a weighted average of the most recent visit counts to a baseline expectation. For the weighted average to be tested, an exponential weighting gives the most influence to the most recent observations. This algorithm is appropriate for daily counts that do not have the characteristic features modeled in the regression algorithm. It is more applicable for Emergency Department data from certain hospital groups and for time series with small counts (daily average below 10) because of the limited case definition or chosen geographic region. The EWMA detection algorithm can be performed with `alert_ewma()` function (run `help(alert_ewma)` or `?alert_ewma` in the R console for more).

```{r alert_ewma}
df_ewma <- alert_ewma(df_hhs, t = date, y = dataCount)
```

Let's visualize the time series with the anomalies for Region 4:

```{r alert_ewma_viz}
# Filter the dataset
df_ewma_region <- df_ewma %>%
  filter(hospitaldhhsregion_display == "Region 4")

# Plot time series data
df_ewma_region %>%
  ggplot() +
  geom_line(aes(x = date, y = dataCount), color = "grey70") +
  geom_line(data = subset(df_ewma_region, alert != "grey"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_ewma_region, alert == "blue"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_ewma_region, alert == "yellow"), aes(x = date, y = dataCount), color = "yellow") +
  geom_point(data = subset(df_ewma_region, alert == "red"), aes(x = date, y = dataCount), color = "red") +
  theme_bw() +
  labs(x = "Date", y = "Count")
```


## Multiple Adaptive Regression
The Multiple Adaptive Regression algorithm fits a linear model to a baseline of counts or percentages, and forecasts a predicted value som predefined days later. This model includes terms to account for linear trends and day-of-week effects. This implementation does NOT include holiday terms as in the Regression 1.2 algorithm in ESSENCE. The EWMA detection algorithm can be performed with the `alert_regression()` function (run `help(alert_regression)` or `?alert_regression` in the R console for more).


```{r alert_regression}
df_regression <- alert_regression(df_hhs, t = date, y = dataCount)
```

Let's visualize the time series with the anomalies for Region 4:

```{r alert_regression_viz}
# Filter the dataset
df_regression_region <- df_regression %>%
  filter(hospitaldhhsregion_display == "Region 4")

# Plot time series data
df_regression_region %>%
  ggplot() +
  geom_line(aes(x = date, y = dataCount), color = "grey70") +
  geom_line(data = subset(df_regression_region, alert != "grey"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_regression_region, alert == "blue"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_regression_region, alert == "yellow"), aes(x = date, y = dataCount), color = "yellow") +
  geom_point(data = subset(df_regression_region, alert == "red"), aes(x = date, y = dataCount), color = "red") +
  theme_bw() +
  labs(x = "Date", y = "Count")
```


## Regression/EWMA Switch
The NSSP-ESSENCE Regression/EWMA Switch algorithm generalized the Regression and EWMA algorithms by applying the most appropriate algorithm for the data in the baseline. First, multiple adaptive regression is applied where the adjusted R squared value of the model is examined to see if it meets a threshold of 0.60. If this threshold is not met, then the model is considered to not explain the data well. In this case, the algorithm switches to the EWMA algorithm, which is more appropriate for sparser time series that are common with county level trends.

The Regression/EWMA algorithm can be performed with the `alert_switch()` function (run `help(alert_switch)` or `?alert_switch` in the R console for more).


```{r alert_switch}
df_switch <- alert_switch(df_hhs, t = date, y = dataCount)
```

Let's visualize the time series with the anomalies for Region 4:

```{r alert_switch_viz}
# Filter the dataset
df_switch_region <- df_switch %>%
  filter(hospitaldhhsregion_display == "Region 4")

# Plot time series data
df_switch_region %>%
  ggplot() +
  geom_line(aes(x = date, y = dataCount), color = "grey70") +
  geom_line(data = subset(df_switch_region, alert != "grey"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_switch_region, alert == "blue"), aes(x = date, y = dataCount), color = "navy") +
  geom_point(data = subset(df_switch_region, alert == "yellow"), aes(x = date, y = dataCount), color = "yellow") +
  geom_point(data = subset(df_switch_region, alert == "red"), aes(x = date, y = dataCount), color = "red") +
  theme_bw() +
  labs(x = "Date", y = "Count")
```


## Farrington Temporal Detector
The Farrington algorithm is intended for weekly time series of counts spanning multiple years. 

The Original Farrington Algorithm uses a quasi-Poisson generalized linear regression models that are fit to baseline counts associated with reference dates in a defined number of previous years, including a defined number of weeks before and after each reference date. 

The Modified implementation of the original Farrington algorithm improves performance by including more historical data in the baseline. 

Both the Original and the Modified Farrington algorithm can be performed with the `alert_farrington()` function. By default, `alert_farrington()` implements the original Farrington. Add `method = "modified"` to the function to apply the modified Farrington algorithm to your data (run `?alert_farrington` in the R console for more).

Let's pull in more historical data:

```{r alert_farrington_data}
url2 <- "https://essence2.syndromicsurveillance.org/nssp_essence/api/timeSeries?endDate=12Feb2022&ccddCategory=cdc%20respiratory%20syncytial%20virus%20v1&percentParam=noPercent&geographySystem=hospitaldhhsregion&datasource=va_hospdreg&detector=nodetectordetector&startDate=29Dec2013&timeResolution=weekly&hasBeenE=1&medicalGroupingSystem=essencesyndromes&userId=2362&aqtTarget=TimeSeries"

api_data2 <- get_api_data(url2, profile = myProfile)

df2 <- api_data2 %>% 
  pluck("timeSeriesData")
```

The Original Farrington temporal detector can be applied as below:

```{r alert_farrington_original}
df_farr_original <- alert_farrington(df2, t = date, y = count)
```

Similarly, the Modified Farrington temporal detector can be applied as below:

```{r alert_farrington_modified}
df_farr_modified <- alert_farrington(
  df2, 
  t = date, 
  y = count, 
  method = "modified"
)
```

Let's visualize the time series with the detected anomalies:

```{r alert_farrington_modified_viz}
df_farr_modified %>%
  ggplot() +
  geom_line(aes(x = date, y = count), color = "grey70") +
  geom_line(data = subset(df_farr_modified, alert != "grey"), aes(x = date, y = count), color = "navy") +
  geom_point(data = subset(df_farr_modified, alert == "blue"), aes(x = date, y = count), color = "navy") +
  geom_point(data = subset(df_farr_modified, alert == "red"), aes(x = date, y = count), color = "red") +
  theme_bw() +
  labs(
    x = "Date",
    y = "Weekly ED Visits"
  )
```

## Negative Binomial Regression
The Negative Binomial Regression algorithm is intended for weekly time series spanning multiple years.

It fits a negative binomial regression model with a time term and order 1 Fourier terms to a baseline period that spans 2 or more years.

Inclusion of Fourier terms in the model is intended to account for seasonality common in multi-year weekly time series of counts. Order 1 sine and cosine terms are included to account for annual seasonality that is common to syndromes and diseases such as influenza, RSV, and norovirus. 

Each baseline model is used to make weekly forecasts for all weeks following the baseline period. One-sided upper 95 computed for each week in the prediction period. 

Alarms are signaled for any week during for which weekly counts fall above the upper bound of the prediction interval.

The Negative Binomial Regression detector can be applied with the `alert_nbinom()` function (run `help(alert_nbinom)` or `?alert_nbinom` in the R console for more).

With the historical data pulled in the previous section, the Negative Binomial Regression detector can be applied as below:

```{r alert_negative_binomial_regression}
df_nbinom <- alert_nbinom(
  df2, 
  t = date, 
  y = count, 
  baseline_end = as.Date("2020-03-01")
)
```

Let's visualize the time series with the detected anomalies:

```{r alert_negative_binomial_regression_viz}
df_nbinom %>%
  ggplot() +
  theme_classic() +
  geom_line(aes(x = date, y = count), linewidth = 0.3) +
  geom_line(aes(x = date, y = estimate), color = "blue", linewidth = 0.3) +
  geom_line(aes(x = date, y = threshold), color = "red", linewidth = 0.3,
            linetype = "dashed") +
  geom_point(data = subset(df_nbinom, alarm),
             aes(x = date, y = count), color = "red", size = 0.7) +
  theme(
   axis.text.x = element_text(angle = 90, vjust = 0.5),
   axis.ticks.length = unit(0.25, "cm")
 ) +
  scale_x_date(
    date_breaks = "6 month",
    date_labels = "%b-%Y"
  ) +
  scale_y_continuous(
    breaks = scales::pretty_breaks(n = 5),
    labels = scales::comma
  ) +
  labs(
    x = "Date",
    y = "Weekly ED Encounters",
    subtitle = "Negative binomial regression algorithm with order-1 Fourier terms"
  ) +
  geom_vline(xintercept = as.Date("2020-03-01"), linetype = "dotted", linewidth = 0.5) +
  annotate(
    geom = "text",
    x = as.Date("2020-03-01") - 270,
    y = Inf, label = "End of baseline\nMarch 1, 2020",
    family = "Candara", vjust = 1.7, size = 3
  )
```

## Original Serfling detector
The Original Serfling detector is intended for weekly time series spanning multiple years.

It fits a linear regression model with a time term and order 1 Fourier terms to a baseline period that ideally spans 5 or more years. 

Inclusion of Fourier terms in the model is intended to account for seasonality common in multi-year weekly time series. Order 1 sine and cosine terms are included to account for annual seasonality that is common to syndromes and diseases such as influenza, RSV, and norovirus. Each baseline model is used to make weekly forecasts for all weeks following the baseline period. One-sided upper 95 for each week in the prediction period. 

Alarms are signaled for any week during for which weekly observations fall above the upper bound of the prediction interval. 

This implementation follows the approach of the original Serfling method in which weeks between October of the starting year of a season and May of the ending year of a season are considered to be in the epidemic period. Weeks in the epidemic period are removed from the baseline prior to fitting the regression model.

The  Original Serfling detector can be applied with the `alert_serfling()` function (run `help(alert_serfling)` or `?alert_serfling` in the R console for more).

With the historical data pulled in the previous section, the Negative Binomial Regression detector can be applied as below:

```{r alert_serfling_detector}
df_serfling <- alert_serfling(
  df2, 
  t = date, 
  y = count, 
  baseline_end = as.Date("2020-03-01")
)
```

Let's visualize the time series with the detected anomalies:

```{r alert_serfling_detector_viz}
df_serfling %>%
  ggplot() +
  theme_classic() +
  geom_line(aes(x = date, y = count), linewidth = 0.3) +
  geom_line(aes(x = date, y = estimate), color = "blue", linewidth = 0.3) +
  geom_line(aes(x = date, y = threshold), color = "red", linewidth = 0.3,
            linetype = "dashed") +
  geom_point(data = subset(df_serfling, alarm), aes(x = date, y = count),
             color = "red", shape = 21, size = 2.5) +
  scale_x_date(
    breaks = seq.Date(from = min(df_serfling$date),
                      to = max(df_serfling$date), by = "4 month"),
    name = "MMWR Week Date"
  ) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5),
    axis.ticks.length = unit(0.25, "cm")
  ) +
  labs(
    title = "Original Serfling Method Results for Weekly ED Encounters",
    subtitle = "Annual seasonality with moderate counts"
  )
```

# Trend Classification
The trend classification fits rolling binomial models to a daily time series of percentages or proportions in order to classify the overall trend during the baseline period as significantly increasing, significantly decreasing, or stable. The algorithm can be performed via the `classify_trend()` function (run `help(classify_trend)` or `?classify_trend` for more). The test statistic and p-value are extracted from each individual model and are used in the following classification scheme:

- p-value < 0.01 and sign(test_statistic) > 0 ~ "Significant Increase" 
- p-value < 0.01 and sign(test_statistic) < 0 ~ "Significant Decrease" 
- p-value >= 0.01 ~ "Stable" 

If there are fewer than 10 encounters/counts in the baseline period, a model is not fit and a value of NA is returned for the test statistic and p-value

```{r classify_trend}
# Detecting trends in HHS region 4
data_trend <- classify_trend(df_hhs, data_count = dataCount, all_count = allCount)
```

Let's visualize HHS region 4 trends with the color bar at the bottom representing the trend classification of each day over time:

```{r classify_trend_viz}
# Filter the dataset
df_trend_region <- data_trend %>%
  filter(hospitaldhhsregion_display == "Region 4")

# Defining a color palette
pal <- c("#FF0000", "#1D8AFF", "#FFF70E", "grey90")

# Plot trend
df_trend_region %>%
  mutate(percent = data_count/all_count * 100) %>%
  ggplot(., aes(x = t, y = percent)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = -0.4, size = 4.5, color = "white") +
  geom_segment(aes(x = t, xend = max(t), y = -0.4, yend = -0.4, 
                   color = trend_classification), size = 3) +
  scale_color_manual(values = pal, name = "Trend Classification") +
  theme_few() +
  labs(title = "Percent of Emergency Department Visits with Diagnosed COVID-19",
       subtitle = "August 22nd, 2020 to November 20th, 2020",
       x = "Date",
       y = "Percent")
```

