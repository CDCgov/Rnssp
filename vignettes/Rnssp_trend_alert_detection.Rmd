---
title: "Anomaly Detection and Trend Classification"
author:
output:
   rmarkdown::html_document:
     toc: true
     toc_float:
       toc_collapsed: true
     toc_depth: 2
     number_sections: true
     theme: readable
vignette: >
  %\VignetteIndexEntry{Anomaly Detection and Trend Classification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "#>",
  collapse = TRUE,
  cache = FALSE
)
```

# Introduction {-}
In this tutorial, we describe how to perform anomaly detection and trend classification analysis using time series data from NSSP-ESSENCE. This vignette uses time series data from NSSP-ESSENCE data source for the CLI CC with CLI DD and Coronavirus DD v2 definition, limiting to ED visits (Has been Emergency = "Yes").

We start this tutorial by loading the `Rnssp` package and all other necessary packages.

```{r setup}
library(Rnssp)
library(ggplot2)
library(ggthemes)
```

The next step is to create an NSSP user profile by creating an object of the class `Credentials`. 

```{r load, echo=FALSE, eval=TRUE}
myProfile <- readRDS("../myProfile.rds")
```


```{r create_profile, eval=FALSE}
# Creating an ESSENCE user profile
myProfile <- Credentials$new(
  username = askme("Enter your username: "), 
  password = askme()
)

# save profile object to file for future use
# save(myProfile, "myProfile.rda") # saveRDS(myProfile, "myProfile.rds")
# Load profile object
# load("myProfile.rda") # myProfile <- readRDS("myProfile.rds")
```

# Data Pull from NSSP-ESSENCE
With the NSSP `myProfile` object, we authenticate to NSSP-ESSENCE and pull in the data using the Time series data table API.
```{r data_pull, collapse=TRUE}
url <- "https://essence2.syndromicsurveillance.org/nssp_essence/api/timeSeries?endDate=20Nov20&ccddCategory=cli%20cc%20with%20cli%20dd%20and%20coronavirus%20dd%20v2&percentParam=ccddCategory&geographySystem=hospitaldhhsregion&datasource=va_hospdreg&detector=nodetectordetector&startDate=22Aug20&timeResolution=daily&hasBeenE=1&medicalGroupingSystem=essencesyndromes&userId=2362&aqtTarget=TimeSeries&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&nonZeroComposite=false&removeZeroSeries=true&startMonth=January&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&startMonth=January&nonZeroComposite=false"

# Data Pull from NSSP-ESSENCE
api_data <- myProfile$get_api_data(url)
df <- api_data$timeSeriesData

# glimpse(df)

```

In this tutorial, we will only show national level trends.
```{r national}
# Aggregating data to national level
df_national <- df %>% 
  group_by(date) %>% 
  summarise(count = sum(dataCount), allCount = sum(allCount))
```

# Anomaly Detection
As of `r format(Sys.Date(), "%B %d, %Y")`, the `Rnssp` package implements two anomaly detection algorithms:

## Exponentially Weighted Moving Average (EWMA)
The Exponentially Weighted Moving Average (EWMA) compares a weighted average of the most recent visit counts to a baseline expectation. For the weighted average to be tested, an exponential weighting gives the most influence to the most recent observations. This algorithm is appropriate for daily counts that do not have the characteristic features modeled in the regression algorithm. It is more applicable for Emergency Department data from certain hospital groups and for time series with small counts (daily average below 10) because of the limited case definition or chosen geographic region. The EWMA detection algorithm can be performed with `alert_ewma()` function (run `help(alert_ewma)` or `?alert_ewma` in the R console for more).

```{r alert_ewma}
df_ewma <- alert_ewma(df_national, t = date, y = count)
```

Let's visualize the National level time series with the anomalies:

```{r alert_ewma_viz}
# Plot time series data
df_ewma %>%
  ggplot(aes(x = t, y = count)) +
  geom_line(color = "blue") +
  geom_point(data = subset(df_ewma, alert == "red"), color = "red") +
  geom_point(data = subset(df_ewma, alert == "yellow"), color = "yellow") +
  theme_bw() +
  labs(x = "Date",
       y = "Percent")
```


## Multiple Adaptive Regression (MAR)
The Multiple Adaptive Regression (MAR) algorithm fits a linear model to a baseline of counts or percentages, and forecasts a predicted value som predefined days later. This model includes terms to account for linear trends and day-of-week effects. This implementation does NOT include holiday terms as in the Regression 1.2 algorithm in ESSENCE. The EWMA detection algorithm can be performed with the `alert_regression()` function (run `help(alert_regression)` or `?alert_regression` in the R console for more).


```{r alert_regression}
df_regression <- alert_regression(df_national, t = date, y = count)
```

Let's visualize the National level time series with the anomalies:

```{r alert_regression_viz}
# Plot time series data
df_regression %>%
  ggplot(aes(x = t, y = count)) +
  geom_line(color = "blue") +
  geom_point(data = subset(df_regression, alert == "red"), color = "red") +
  geom_point(data = subset(df_regression, alert == "yellow"), color = "yellow") +
  theme_bw() +
  labs(x = "Date",
       y = "Percent")
```

## Farrington Temporal Detector
The Farrington algorithm is intended for weekly time series of counts spanning multiple years. 

The Original Farrington Algorithm uses a quasi-Poisson generalized linear regression models that are fit to baseline counts associated with reference dates in a defined number of previous years, including a defined number of weeks before and after each reference date. 

The Modified implementation of the original Farrington algorithm improves performance by including more historical data in the baseline. 

Both the Original and the Modified Farrington algorithm can be performed with the `alert_farrington()` function. By default, `alert_farrington()` implements the original Farrington. Add `method = "modified"` to the function to apply the modified Farrington algorithm to your data (run `?alert_farrington` in the R console for more).

Let's pull in more historical data:

```{r alert_farrington_data}
url2 <- "https://essence2.syndromicsurveillance.org/nssp_essence/api/timeSeries?endDate=20Nov20&percentParam=ccddCategory&datasource=va_hosp&startDate=22Aug20&medicalGroupingSystem=essencesyndromes&userId=2362&aqtTarget=TimeSeries&ccddCategory=cli%20cc%20with%20cli%20dd%20and%20coronavirus%20dd%20v2&geographySystem=hospitalstate&detector=probregv2&timeResolution=daily&hasBeenE=1&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&nonZeroComposite=false&removeZeroSeries=true&sigDigits=true&startMonth=January&stratVal=&multiStratVal=geography&graphOnly=true&numSeries=0&graphOptions=multipleSmall&seriesPerYear=false&startMonth=January&nonZeroComposite=false"

api_data2 <- get_api_data(url2)

df2 <- api_data$timeSeriesData
```

The Original Farrington temporal detector can be applied as below:

```{r alert_farrington_original}
df_farr_original <- alert_farrington(df2, t = date, y = count)
```

Similarly, the Modified Farrington temporal detector can be applied as below:

```{r alert_farrington_modified}
df_farr_modified <- alert_farrington(df, t = date, y = count, method = "modified")
```

Let's visualize the time series with the detected anomalies:

```{r alert_farrington_modified_viz}
df_farr_modified %>%
  ggplot() +
  geom_line(aes(x = date, y = count), size = 0.4, color = "grey70") +
  geom_line(data = subset(df_farr_modified, !is.na(alert)), aes(x = date, y = count), size = 0.4, color = "navy") +
  geom_point(data = subset(df_farr_modified, !alert), aes(x = date, y = count), size = 1.5, color = "navy") +
  geom_point(data = subset(df_farr_modified, alert), aes(x = date, y = count), color = "red", size = 1.5) +
  theme_bw() +
  labs(
    x = "Date",
    y = "Weekly ED Visits"
  )
```

# Trend Classification
The trend classification fits rolling binomial models to a daily time series of percentages or proportions in order to classify the overall trend during the baseline period as significantly increasing, significantly decreasing, or stable. The algorithm can be performed via the `classify_trend()` function (run `help(classify_trend)` or `?classify_trend` for more). The test statistic and p-value are extracted from each individual model and are used in the following classification scheme:

- p-value < 0.01 and sign(test_statistic) > 0 ~ "Significant Increase" 
- p-value < 0.01 and sign(test_statistic) < 0 ~ "Significant Decrease" 
- p-value >= 0.01 ~ "Stable" 

If there are fewer than 10 encounters/counts in the baseline period, a model is not fit and a value of NA is returned for the test statistic and p-value

```{r classify_trend}
data_trend <- classify_trend(df_national, data_count = count, all_count = allCount)
```

Let's visualize the National trends with the color bar at the bottom representing the trend classification of each day over time:

```{r classify_trend_viz}
# Defining a color palette
pal <- c("#FF0000", "#1D8AFF", "#FFF70E", "grey90")

# Plot trend
data_trend %>%
  mutate(percent = data_count/all_count * 100) %>%
  ggplot(., aes(x = t, y = percent)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = -0.4, size = 4.5, color = "white") +
  geom_segment(aes(x = t, xend = max(t), y = -0.4, yend = -0.4, 
                   color = trend_classification), size = 3) +
  scale_color_manual(values = pal, name = "Trend Classification") +
  theme_few() +
  labs(title = "Percent of Emergency Department Visits with Diagnosed COVID-19",
       subtitle = "August 22nd, 2020 to November 20th, 2020",
       x = "Date",
       y = "Percent")
```

